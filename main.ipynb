{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70b32e9e-c8ca-4222-a994-3000ea8ee99a",
   "metadata": {},
   "source": [
    "# STT와 TTS를 결합한 소음 제거 모델 (Noise Reduction Model Combining STT and TTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0d7e0f-cb09-47df-87fd-4871b623a892",
   "metadata": {},
   "source": [
    "- 아래 코드는 논문에서 구현된 모델을 직접 테스트해볼 수 있도록 구현되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf23946-6a8c-47be-830d-5d8f0fd86076",
   "metadata": {},
   "source": [
    "# 1. Load STT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1279a0f5-3e70-4980-98e2-c50a89b46e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, WhisperFeatureExtractor, WhisperTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "import librosa\n",
    "import openai\n",
    "openai.api_key = \"(Insert your api key)\" # OpenAI API key를 \" \" 사이에 넣어주세요.\n",
    "\n",
    "import time\n",
    "from gtts import gTTS\n",
    "import IPython.display as ipd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "52d51465-1f02-46b1-8918-ae75b17023da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WhisperForConditionalGeneration(\n",
       "  (model): WhisperModel(\n",
       "    (encoder): WhisperEncoder(\n",
       "      (conv1): Conv1d(80, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (conv2): Conv1d(768, 768, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (embed_positions): Embedding(1500, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x WhisperEncoderLayer(\n",
       "          (self_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): WhisperDecoder(\n",
       "      (embed_tokens): Embedding(51865, 768, padding_idx=50257)\n",
       "      (embed_positions): WhisperPositionalEmbedding(448, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x WhisperDecoderLayer(\n",
       "          (self_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (proj_out): Linear(in_features=768, out_features=51865, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = WhisperForConditionalGeneration.from_pretrained(\"Taeyeun72/whisper-small-noising_6\") # Final Model\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"Korean\", task=\"transcribe\")\n",
    "\n",
    "forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"korean\", task=\"transcribe\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005b38c8-d78d-4040-9ed4-a0121558d42a",
   "metadata": {},
   "source": [
    "# 2. Build Model Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "332b1655-3978-448c-8190-961638a0640d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def DNS(audio_path):\n",
    "    directory, filename = os.path.split(audio_path)\n",
    "    audio_dns_path = directory + \"/dns/\" + filename\n",
    "    command = f\"python ./noise-reduction-master/denoise.py --model=./noise-reduction-master/models/tscn --noisy={audio_path} --denoise={audio_dns_path}\"\n",
    "    # 명령어 실행\n",
    "    exit_code = os.system(command)\n",
    "    # 종료 코드 확인\n",
    "    if exit_code != 0:\n",
    "        print(f\"Error: [Errno {exit_code}]\")\n",
    "        print(\"DNS: Fail!\")\n",
    "    else:\n",
    "        print(\"DNS: Done!\")\n",
    "    return \"audio/\" + \"dns/\" + filename\n",
    "\n",
    "def STT(audio_dns_path, model=model):\n",
    "    y, s = librosa.load(audio_dns_path, sr=16000) # sampling rate 변경 (44100 -> 16000)\n",
    "    input_features = processor(y, sampling_rate=16000, return_tensors=\"pt\").input_features\n",
    "    predicted_ids = model.generate(input_features.to('cuda'), forced_decoder_ids=forced_decoder_ids)\n",
    "    predicted = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "    print(f\"STT result: {predicted}\")\n",
    "    return predicted\n",
    "\n",
    "def GPT(text):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"주어진 문장에서 틀린 글자와 단어를 맥락에 맞게 모두 고치세요.\"},\n",
    "            {\"role\": \"system\", \"content\": \"구두점(마침표, 쉼표, 느낌표, 물음표)이 표기되어 있지 않으면 구두점을 추가로 표기하세요.\"},\n",
    "            {\"role\": \"system\", \"content\": \"단, 고친 문장만을 출력하세요. 다른 어떠한 부가적인 문장 추가하지 마세요.\"},\n",
    "            {\"role\": \"user\", \"content\": text},\n",
    "        ],\n",
    "        temperature=0.1, # temperature 설정 (높은 값: 출력이 무작위로 생성, 낮은 값: 출력이 일관성 있게 생성)\n",
    "    )\n",
    "\n",
    "    output_text = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    print(f\"ChatGPT result: {output_text}\")\n",
    "    return output_text\n",
    "\n",
    "def TTS(text, audio_path):\n",
    "    directory, filename = os.path.split(audio_path)\n",
    "    save_path = \"audio/\" + \"result/\" + filename\n",
    "    tts = gTTS(text=text, lang='ko')\n",
    "    tts.save(save_path)\n",
    "    print(\"TTS result: Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "95481cbe-b17a-4522-8faa-e9ece6340ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(audio_path, model=model, GPT_flag=True):\n",
    "    start = time.time()\n",
    "    \n",
    "    # DNS\n",
    "    audio_dns_path = DNS(audio_path)\n",
    "\n",
    "    # STT\n",
    "    text = STT(audio_dns_path, model)\n",
    "\n",
    "    # GPT\n",
    "    if GPT_flag == True:\n",
    "        text = GPT(text)\n",
    "\n",
    "    # TTS\n",
    "    TTS(text, audio_path)\n",
    "    \n",
    "    end = time.time()\n",
    "    print(f\"Inference time: {end-start:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "88626242-e8c8-46eb-ad59-43151bf861c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNS: Done!\n",
      "STT result: 최근 블루투스 이어폰의 사용의 증가함에 따라 이어폰을 착용하며 통화하는 사람들이 늘어나고 있다. 그러나 블루투스 이어폰의 특성상 주변 소음을 많이 포함하기도 해 통화 품질의 영향을 미치게 된다.\n",
      "ChatGPT result: 최근 블루투스 이어폰의 사용이 증가함에 따라 이어폰을 착용하며 통화하는 사람들이 늘어나고 있다. 그러나 블루투스 이어폰의 특성상 주변 소음을 많이 포함하기도 해 통화 품질에 영향을 미치게 된다.\n",
      "TTS result: Done!\n",
      "Inference time: 11.9\n"
     ]
    }
   ],
   "source": [
    "main(\"Insert your audio path\") # 오디오 wav 파일을 audio 폴더에 넣고 \" \"에 전체 경로를 넣어주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfdc960-6c87-4bd5-9e87-125c67e4dc0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
